[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "O You Who Believe",
    "section": "",
    "text": "Welcome to OYWB\nThis is the website for O You Who Believe, which is a textbook from the OpenIntro project.\n::: under construction OYWB is currently under construction, it’s planned to be released TBA. The first edition of the book is available TBA. If you have any feedback on the work-in-progress first edition or catch any typos, we would appreciate you reporting at github.com/aryr100/ims2.\n\n\nCopyright © 2023.\nSecond Edition.\nVersion date: 1.0.\nThis textbook is available under a Creative Commons Attribution-ShareAlike 4.0 Unported United States License. License details are available at the Creative Commons website:\ncreativecommons.org. :::",
    "crumbs": [
      "Welcome to OYWB"
    ]
  },
  {
    "objectID": "content/authors.html",
    "href": "content/authors.html",
    "title": "Authors",
    "section": "",
    "text": "Mine Çetinkaya-Rundel\nDuke University, RStudio\nmine@openintro.org\n\nMine Çetinkaya-Rundel is Professor of the Practice at the Department of Statistical Science at Duke University and Developer Educator at Posit. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM. Mine works on integrating computation into the undergraduate statistics curriculum, using reproducible research methodologies and analysis of real and complex datasets. She also organizes ASA DataFest, an annual two-day competition in which teams of undergraduate students work to reveal insights into a rich and complex dataset. Mine has been working on the OpenIntro project since its founding and as part of this project she co-authored four open-source introductory statistics textbooks (including this one!). She is also the creator and maintainer of datasciencebox.org and she teaches the popular Statistics with R MOOC on Coursera. \n\n\n Johanna Hardin\nPomona College\njo.hardin@pomona.edu\n\nJo Hardin is Professor of Mathematics and Statistics at Pomona College. She collaborates with molecular biologists to create novel statistical methods for analyzing high throughput data. She has also worked extensively in statistics and data science education, facilitating modern curricula for higher education instructors. She was a co-author on the 2014 ASA Curriculum Guidelines for Undergraduate Programs in Statistical Science, and she writes on the blog teachdatascience.com. The best part of her job is collaborating with undergraduate students. In her spare time, she loves reading, running, and breeding tortoises.",
    "crumbs": [
      "Authors"
    ]
  },
  {
    "objectID": "content/preface.html",
    "href": "content/preface.html",
    "title": "Preface",
    "section": "",
    "text": "We hope readers will take away three ideas from this book in addition to forming a foundation of statistical thinking and methods.\n\nStatistics is an applied field with a wide range of practical applications.\nYou don’t have to be a math guru to learn from interesting, real data.\nData are messy, and statistical tools are imperfect. However, when you understand the strengths and weaknesses of these tools, you can use them to learn interesting things about the world.\n\n\nTextbook overview\n\nPart 1: Introduction to data. Data structures, variables, summaries, graphics, and basic data collection and study design techniques.\nPart 2: Exploratory data analysis. Data visualization and summarization, with particular emphasis on multivariable relationships.\nPart 3: Regression modeling. Modeling numerical and categorical outcomes with linear and logistic regression and using model results to describe relationships and make predictions.\nPart 4: Foundations for inference. Case studies are used to introduce the ideas of statistical inference with randomization tests, bootstrap intervals, and mathematical models.\nPart 5: Statistical inference. Further details of statistical inference using randomization tests, bootstrap intervals, and mathematical models for numerical and categorical data.\nPart 6: Inferential modeling. Extending inference techniques presented thus-far to linear and logistic regression settings and evaluating model performance.\n\nEach part contains multiple chapters and ends with a case study. Building on the content covered in the part, the case study uses the tools and techniques to present a high-level overview.\nEach chapter ends with a review section which contains a chapter summary as well as a list of key terms introduced in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We purposefully present them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However, you should be able to easily spot them as bolded text.\n\n\nExamples and exercises\nExamples are provided to establish an understanding of how to apply methods.\n\nThis is an example. When a question is asked here, where can the answer be found?\n\nThe answer can be found here, in the solution section of the example!\n\nWhen we think the reader is ready to try determining a solution on their own, we frame it as Guided Practice.\n\nThe reader may check or learn the answer to any Guided Practice problem by reviewing the full solution in a footnote.1\n\n\n\nDatasets and their sources\nA large majority of the datasets used in the book can be found in various R packages. Each time a new dataset is introduced in the narrative, a reference to the package like the one below is provided. Many of these datasets are in the openintro R package that contains datasets used in OpenIntro’s open-source textbooks.[^preface-2]\n\nThe textbooks data can be found in the openintro R package.\n\nThe datasets used throughout the book come from real sources like opinion polls and scientific articles, except for a handful of cases where we use toy data to highlight a particular feature or explain a particular concept. References for the sources of the real data are provided at the end of the book.\n\n\nComputing with R\nThe narrative and the exercises in the book are computing language agnostic, however while it’s possible to learn about modern statistics without computing, it’s not possible to apply it. Therefore, we invite you to navigate the concepts you have learned in each part using the interactive R tutorials and the R labs that are included at the end of each part.\nInteractive R tutorials\nThe self-paced and interactive R tutorials were developed using the learnr R package, and only an internet browser is needed to complete them.\n\nEach part comes with a tutorial comprised of 4-10 lessons and listed like this.\n\n\nEach of these lessons…\n\n\n… is listed like this.\n\nYou can access the full list of tutorials supporting this book here.\nR labs\nOnce you feel comfortable with the material in the tutorials, we also encourage you to apply what you’ve learned via the computational labs that are also linked at the end of each part. The labs consist of data analysis case studies, and they require access to R and RStudio. The first lab includes installation instructions. If you’d rather not install the software locally, you can also try RStudio Cloud for free.\n\nLabs for each part are listed like this.\n\nYou can access the full list of labs supporting this book here.\n\n\nOpenIntro, online resources, and getting involved\nOpenIntro is an organization focused on developing free and affordable education materials.\nWe encourage anyone learning or teaching statistics to visit openintro.org and to get involved.\nAll OpenIntro resources are free and anyone is welcomed to use these online tools and resources with or without this textbook as a companion.\nWe value your feedback. If there is a part of the project you especially like or think needs improvement, we want to hear from you. For feedback on this specific book, you can open an issue on GitHub. You can also provide feedback on this book or any other OpenIntro resource via our contact form at openintro.org.\n\n\nAcknowledgements\nThe OpenIntro project would not have been possible without the dedication and volunteer hours of all those involved, and we hope you will join us in extending a huge thank you to all those who volunteer with OpenIntro.\nWe would like to also thank the developers of the open-source tools that make the development and authoring of this book possible, e.g., bookdown, tidyverse, and icons8.\nWe are also grateful to the many teachers, students, and other readers who have helped improve OpenIntro resources through their feedback.\n\n\n\n\n\n\nGuided Practice problems are intended to stretch your thinking, and you can check yourself by reviewing the footnote solution for any Guided Practice.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "content/introduction-to-data.html",
    "href": "content/introduction-to-data.html",
    "title": "Introduction to data",
    "section": "",
    "text": "In this part of the book, you will be introduced to data, their properties, how they are collected, and the structure of the design used for the study. Different data and settings lead to different types of conclusions, so you’ll always want to keep in mind the data provenance, especially as you move on to modeling and inference.\n\nIn Chapter 1 you’ll be introduced to tidy data, an important structure for describing, visualizing, and analyzing data.\nIn Chapter 2 the focus is on study design. In particular, the critical distinction between random sampling and randomization is made.\nChapter 3 includes an application on the Paralympics case study where the topics from this part of the book are fully developed.\n\nWe recommend you come back to this part to review after you cover a new part in the textbook. In particular, it is worthwhile to consider ?fig-randsampValloc in all of the inferential settings you cover. Each dataset you analyze will have a slightly different context which will require thoughtful consideration of the appropriate conclusions.",
    "crumbs": [
      "Introduction to data"
    ]
  },
  {
    "objectID": "content/01-data-hello.html#sec-case-study-stents-strokes",
    "href": "content/01-data-hello.html#sec-case-study-stents-strokes",
    "title": "1  Hello data",
    "section": "1.1 Case study: Using stents to prevent strokes",
    "text": "1.1 Case study: Using stents to prevent strokes\nIn this section we introduce a classic challenge in statistics: evaluating the efficacy of a medical treatment. Terms in this section, and indeed much of this chapter, will all be revisited later in the text. The plan for now is simply to get a sense of the role statistics can play in practice.\nAn experiment is designed to study the effectiveness of stents in treating patients at risk of stroke (Chimowitz et al. 2011). Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death.\nMany doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer:\n\nDoes the use of stents reduce the risk of stroke?\n\nThe researchers who asked this question conducted an experiment with 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups:\n\nTreatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification.\nControl group. Patients in the control group received the same medical management as the treatment group, but they did not receive stents.\n\nResearchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group.\nResearchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. The results of 5 patients are summarized in. Patient outcomes are recorded as stroke or no event, representing whether the patient had a stroke during that time period.\n\nThe stent30 data and stent365 data can be found in the openintro R package.\n\n\nOf the 224 patients in the treatment group, 45 had a stroke by the end of the first year. Using these two numbers, compute the proportion of patients in the treatment group who had a stroke by the end of their first year. (Note: answers to all Guided Practice exercises are provided in footnotes!)1\n\nWe can compute summary statistics from the table to give us a better idea of how the impact of the stent treatment differed between the two groups. A summary statistic is a single number summarizing data from a sample. For instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups.\n\nProportion who had a stroke in the treatment (stent) group: \\(45/224 = 0.20 = 20\\%.\\)\nProportion who had a stroke in the control group: \\(28/227 = 0.12 = 12\\%.\\)\n\nThese two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 8% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a “real” difference between the groups?\nThis second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of variation is part of almost any type of data generating process. It is possible that the 8% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So, what we are really asking is the following: if in fact stents have no effect, how likely is it that we observe such a large difference?\nWhile we do not yet have statistical tools to fully address this question on our own, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients.\nBe careful: Do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents, and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hello data</span>"
    ]
  },
  {
    "objectID": "content/01-data-hello.html#sec-data-basics",
    "href": "content/01-data-hello.html#sec-data-basics",
    "title": "1  Hello data",
    "section": "1.2 Data basics",
    "text": "1.2 Data basics\nEffective presentation and description of data is a first step in most analyses. This section introduces one structure for organizing data as well as some terminology that will be used throughout this book.\n\n1.2.1 Observations, variables, and data matrices\n(loan50-df?) displays six rows of a dataset for 50 randomly sampled loans offered through Lending Club, which is a peer-to-peer lending company. This dataset will be referred to as loan50.\n\nThe loan50 data can be found in the openintro R package.\n\nEach row in the table represents a single loan. The formal name for a row is a case or observational unit. The columns represent characteristics of each loan, where each column is referred to as a variable. For example, the first row represents a loan of $22,000 with an interest rate of 10.90%, where the borrower is based in New Jersey (NJ) and has an income of $59,000.\n\nWhat is the grade of the first loan in ? And what is the home ownership status of the borrower for that first loan? Reminder: for these Guided Practice questions, you can check your answer in the footnote.2\n\nIn practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and its units of measurement. Descriptions of the variables in the loan50 dataset are given in.\n\nThe grades for assignments, quizzes, and exams in a course are often recorded in a gradebook that takes the form of a data frame. How might you organize a course’s grade data using a data frame? Describe the observational units and variables.3\n\n\nWe consider data for 3,142 counties in the United States, which includes the name of each county, the state where it resides, its population in 2017, the population change from 2010 to 2017, poverty rate, and nine additional characteristics. How might these data be organized in a data frame?4\n\nThe data described in the Guided Practice above represents the county dataset, which is shown as a data frame in . The variables as well as the variables in the dataset that did not fit in are described in.\n\nThe county data can be found in the usdata R package.\n\n\n\n1.2.2 Types of variables {variable-types}\nExamine the unemployment_rate, pop2017, state, and median_edu variables in the county dataset. Each of these variables is inherently different from the other three, yet some share certain characteristics.\nFirst consider unemployment_rate, which is said to be a numerical variable since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as numerical since the average, sum, and difference of area codes does not have any clear meaning. Instead, we would consider area codes as a categorical variable.\nThe pop2017 variable is also numerical, although it seems to be a little different than unemployment_rate. This variable of the population count can only take whole non-negative numbers (0, 1, 2, …). For this reason, the population variable is said to be discrete since it can only take numerical values with jumps. On the other hand, the unemployment rate variable is said to be continuous.\nThe variable state can take up to 51 values after accounting for Washington, DC: Alabama, Alaska, …, and Wyoming. Because the responses themselves are categories, state is called a categorical variable, and the possible values (states) are called the variable’s levels (e.g., District of Columbia, Alabama, Alaska, etc.) .\nFinally, consider the median_edu variable, which describes the median education level of county residents and takes values below_hs, hs_diploma, some_college, or bachelors in each county. This variable seems to be a hybrid: it is a categorical variable, but the levels have a natural ordering. A variable with these properties is called an ordinal variable, while a regular categorical variable without this type of special ordering is called a nominal variable. To simplify analyses, any categorical variable in this book will be treated as a nominal (unordered) categorical variable.\n\nData were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous numerical, discrete numerical, or categorical.\n\nThe number of siblings and student height represent numerical variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous numerical variable. The last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical.\n\n\nAn experiment is evaluating the effectiveness of a new drug in treating migraines. A group variable is used to indicate the experiment group for each patient: treatment or control. The num_migraines variable represents the number of migraines the patient experienced during a 3-month period. Classify each variable as either numerical or categorical?5\n\n\n\n1.2.3 Relationships between variables {variable-relations}\nMany analyses are motivated by a researcher looking for a relationship between two or more variables. A social scientist may like to answer some of the following questions:\n\nDoes a higher-than-average increase in county population tend to correspond to counties with higher or lower median household incomes?\n\n\nIf homeownership in one county is lower than the national average, will the percent of housing units that are in multi-unit structures in that county tend to be above or below the national average?\n\n\nHow much can the median education level explain the median household income for counties in the US?\n\nTo answer these questions, data must be collected, such as the county dataset shown in . Examining summary statistics can provide numerical insights about the specifics of each of these questions. Alternatively, graphs can be used to visually explore the data, potentially providing more insight than a summary statistic.\nScatterplots are one type of graph used to study the relationship between two numerical variables. ?fig-county-multi-unit-homeownership displays the relationship between the variables homeownership and multi_unit, which is the percent of housing units that are in multi-unit structures (e.g., apartments, condos). Each point on the plot represents a single county. For instance, the highlighted dot corresponds to County 413 in the county dataset: Chattahoochee County, Georgia, which has 39.4% of housing units that are in multi-unit structures and a homeownership rate of 31.3%. The scatterplot suggests a relationship between the two variables: counties with a higher rate of housing units that are in multi-unit structures tend to have lower homeownership rates. We might brainstorm as to why this relationship exists and investigate each idea to determine which are the most reasonable explanations.\n\nExamine the variables in the loan50 dataset, which are described in. Create two questions about possible relationships between variables in loan50 that are of interest to you.6\n\n\nThis example examines the relationship between the percent change in population from 2010 to 2017 and median household income for counties, which is visualized as a scatterplot in. Are these variables associated?\n\nThe larger the median household income for a county, the higher the population growth observed for the county. While it isn’t true that every county with a higher median household income has a higher population growth, the trend in the plot is evident. Since there is some relationship between the variables, they are associated.\n\n\nAssociated or independent, not both.\nA pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent.\n\n\n\n1.2.4 Explanatory and response variables\nWhen we ask questions about the relationship between two variables, we sometimes also want to determine if the change in one variable causes a change in the other. Consider the following rephrasing of an earlier question about the county dataset:\n\nIf there is an increase in the median household income in a county, does this drive an increase in its population?\n\nIn this question, we are asking whether one variable affects another. If this is our underlying belief, then median household income is the explanatory variable, and the population change is the response variable in the hypothesized relationship.7\n\nExplanatory and response variables.\nWhen we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable. We also use the terms explanatory and response to describe variables where the response might be predicted using the explanatory even if there is no causal relationship.\n\nexplanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable\n\n For many pairs of variables, there is no hypothesized relationship, and these labels would not be applied to either variable in such cases.\n\nBear in mind that the act of labeling the variables in this way does nothing to guarantee that a causal relationship exists. A formal evaluation to check whether one variable causes a change in another requires an experiment.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hello data</span>"
    ]
  },
  {
    "objectID": "content/01-data-hello.html#ayat-104",
    "href": "content/01-data-hello.html#ayat-104",
    "title": "1  Hello data",
    "section": "1.3 Ayat 104",
    "text": "1.3 Ayat 104\n\n\n\n\nChimowitz, Marc I, Michael J Lynn, Colin P Derdeyn, Tanya N Turan, David Fiorella, Bethany F Lane, L Scott Janis, et al. 2011. “Stenting Versus Aggressive Medical Therapy for Intracranial Arterial Stenosis.” New England Journal of Medicine 365 (11): 993–1003. http://www.nejm.org/doi/full/10.1056/NEJMoa1105335.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hello data</span>"
    ]
  },
  {
    "objectID": "content/02-data-design.html#sec-sampling-principles-strategies",
    "href": "content/02-data-design.html#sec-sampling-principles-strategies",
    "title": "2  Study design",
    "section": "2.1 Sampling principles and strategies",
    "text": "2.1 Sampling principles and strategies\nThe first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that the data are reliable and help achieve the research goals.\n\n2.1.1 Populations and samples\nConsider the following three research questions:\n\nWhat is the average mercury content in swordfish in the Atlantic Ocean?\nOver the last five years, what is the average time to complete a degree for Duke undergrads?\nDoes a new drug reduce the number of deaths in patients with severe heart disease?\n\nEach research question refers to a target population. In the first question, the target population is all swordfish in the Atlantic Ocean, and each fish represents a case. Oftentimes, it is not feasible to collect data for every case in a population. Collecting data for an entire population is called a census. A census is difficult because it is too expensive to collect data for the entire population, but it might also be because it is difficult or impossible to identify the entire population of interest! Instead, a sample is taken. A sample is the data you have. Ideally, a sample is a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and to answer the research question.\n\nFor the second and third questions above, identify the target population and what represents an individual case.1\n\n\n\n2.1.2 Parameters and statistics\nIn most statistical analysis procedures, the research question at hand boils down to understanding a numerical summary. The number (or set of numbers) may be a quantity you are already familiar with (like the average) or it may be something you learn through this text (like the slope and intercept from a least squares model, provided in ?sec-least-squares-regression).\nA numerical summary can be calculated on either the sample of observations or the entire population. However, measuring every unit in the population is usually prohibitive (so the parameter is very rarely calculated). So, a “typical” numerical summary is calculated from a sample. Yet, we can still conceptualize calculating the average income of all adults in Argentina.\nWe use specific terms in order to differentiate when a number is being calculated on a sample of data (statistic) and when it is being calculated or considered for calculation on the entire population (parameter). The terms statistic and parameter are useful for communicating claims and models and will be used extensively in later chapters which delve into making inference on populations.\n\n\n2.1.3 Anecdotal evidence\n\nConsider the following possible responses to the three research questions:\n\nA man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high.\nI met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges.\nMy friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work.\n\nEach conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence.\n\nAnecdotal evidence.\nBe careful of data collected in a haphazard fashion. Such evidence may be true and verifiable, but it may only represent extraordinary cases and therefore not be a good representation of the population.\n\nAnecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead, of looking at the most unusual cases, we should examine a sample of many cases that better represent the population.\n\n\n2.1.4 Sampling from a population\n \nWe might try to estimate the time to graduation for Duke undergraduates in the last five years by collecting a sample of graduates. All graduates in the last five years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 10 tickets. The selected names would represent a random sample of 10 graduates.\n\nSuppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think they might collect? Do you think their sample would be representative of all graduates?\n\nPerhaps they would pick a disproportionate number of graduates from health-related fields. Or perhaps their selection would be a good representation of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if our bias is unintended.\n\nThe act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g., for surveys, caution must be exercised if the non-response rate is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are representative of the entire population. This non-response bias can skew results.\n\nWe can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product? Why or why not?2\n\n \n\n\n2.1.5 Four sampling methods\nAlmost all statistical methods are based on the notion of implied randomness. If data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable. Here we consider four random sampling techniques: simple, stratified, cluster, and multistage sampling. Figures ?fig-simple-stratified and ?fig-cluster-multistage provide graphical representations of these techniques.\n \nSimple random sampling is probably the most intuitive form of random sampling. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries, we could write the names of that season’s several hundreds of players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included.\n\nStratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, each of the 30 teams could represent a stratum, since some teams have a lot more money (up to 4 times as much!). Then we might randomly sample 4 players from each team for our sample of 120 players.\n\nWhy would it be good for cases within each stratum to be very similar?\n\nWe might get a more stable estimate for the subpopulation in a stratum if the cases are very similar, leading to more precise estimates within each group. When we combine these estimates into a single estimate for the full population, that population estimate will tend to be more precise since each individual group estimate is itself more precise.\n\nIn a cluster sample, we break up the population into many groups, called clusters. Then we sample a fixed number of clusters and include all observations from each of those clusters in the sample. A multistage sample is like a cluster sample, but rather than keeping all observations in each cluster, we would collect a random sample within each selected cluster.\nSometimes cluster or multistage sampling can be more economical than the alternative sampling techniques. Also, unlike stratified sampling, these approaches are most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves do not look very different from one another. For example, if neighborhoods represented clusters, then cluster or multistage sampling work best when the populations inside each neighborhood are very diverse. A downside of these methods is that more advanced techniques are typically required to analyze the data, though the methods in this book can be extended to handle such data.\n\nSuppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less like the next, but the distances between the villages are substantial. Our goal is to test 150 individuals for malaria. What sampling method should be employed?\n\nA simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling or multistage sampling seem like very good ideas. If we decided to use multistage sampling, we might randomly select half of the villages, then randomly select 10 people from each. This would probably reduce our data collection costs substantially in comparison to a simple random sample, and the cluster sample would still give us reliable information, even if we would need to analyze the data with slightly more advanced methods than we discuss in this book.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "content/02-data-design.html#sec-experiments",
    "href": "content/02-data-design.html#sec-experiments",
    "title": "2  Study design",
    "section": "2.2 Experiments",
    "text": "2.2 Experiments\nStudies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g., using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables.\n\n2.2.1 Principles of experimental design\n\nControlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups3. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may instruct every patient to drink a 12-ounce glass of water with the pill.\n\n\nRandomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. In this example dietary habit is a confounding variable4, which is defined as a variable that is associated with both the explanatory and response variables. Randomizing patients into the treatment or control group helps even out such differences.\nReplication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. What is considered sufficiently large varies from experiment to experiment, but at a minimum we want to have multiple subjects (experimental units) per treatment group. Another way of achieving replication is replicating an entire study to verify an earlier finding. The term replication crisis refers to the ongoing methodological crisis in which past findings from scientific studies in several disciplines have failed to be replicated. Pseudoreplication occurs when individual observations under different treatments are heavily dependent on each other. For example, suppose you have 50 subjects in an experiment where you’re taking blood pressure measurements at 10 time points throughout the course of the study. By the end, you will have 50 \\(\\times\\) 10 = 500 measurements. Reporting that you have 500 observations would be considered pseudoreplication, as the blood pressure measurements of a given individual are not independent of each other. Pseudoreplication often happens when the wrong entity is replicated, and the reported sample sizes are exaggerated.\n\n\nBlocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in ?fig-blocking. This strategy ensures that each treatment group has the same number of low-risk patients and the same number of high-risk patients.\n\nIt is important to incorporate the first three experimental design principles into any study, and this book describes applicable methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this book may be extended to analyze data collected using blocking.\n\n\n2.2.2 Reducing bias in human experiments\nRandomized experiments have long been considered to be the gold standard for data collection, but they do not ensure an unbiased perspective into the cause-and-effect relationship in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients. In particular, researchers wanted to know if the drug reduced deaths in patients.\nThese researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers5 were randomly placed into two study groups. One group, the treatment group, received the drug. The other group, called the control group, did not receive any drug treatment.\nPut yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group does not receive the drug and sits idly, hoping her participation does not increase her risk of death. These perspectives suggest there are actually two effects in this study: the one of interest is the effectiveness of the drug, and the second is an emotional effect of (not) taking the drug, which is difficult to quantify.\nResearchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient does not receive a treatment, they will know they’re in the control group. A solution to this problem is to give a fake treatment to patients in the control group. This is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. However, offering such a fake treatment may not be ethical in certain experiments. For example, in medical experiments, typically the control group must get the current standard of care. Oftentimes, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect.\nThe patients are not the only ones who should be blinded: doctors and researchers can unintentionally bias a study. When a doctor knows a patient has been given the real treatment, they might inadvertently give that patient more attention or care than a patient that they know is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.6\n\nLook back to the study in Section 1.1 where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?7\n\n\nFor the study in Section 1.1, could the researchers have employed a placebo? If so, what would that placebo have looked like?8\n\nYou may have many questions about the ethics of sham surgeries to create a placebo. These questions may have even arisen in your mind when in the general experiment context, where a possibly helpful treatment was withheld from individuals in the control group; the main difference is that a sham surgery tends to create additional risk, while withholding a treatment only maintains a person’s risk.\nThere are always multiple viewpoints of experiments and placebos, and rarely is it obvious which is ethically “correct”. For instance, is it ethical to use a sham surgery when it creates a risk to the patient? However, if we do not use sham surgeries, we may promote the use of a costly treatment that has no real effect; if this happens, money and other resources will be diverted away from other treatments that are known to be helpful. Ultimately, this is a difficult situation where we cannot perfectly protect both the patients who have volunteered for the study and the patients who may benefit (or not) from the treatment in the future.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "content/02-data-design.html#sec-observational-studies",
    "href": "content/02-data-design.html#sec-observational-studies",
    "title": "2  Study design",
    "section": "2.3 Observational studies",
    "text": "2.3 Observational studies\nData where no treatment has been explicitly applied (or explicitly withheld) is called observational data. For instance, the loan data and county data described in Section 1.2 are both examples of observational data.\nMaking causal conclusions based on experiments is often reasonable, since we can randomly assign the explanatory variable(s), i.e., the treatments. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations or form hypotheses that can be later checked with experiments.\n\nSuppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?9\n\nSome previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, they are more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple observational investigation.\nIn this example, sun exposure is a confounding variable. The presence of confounding variables is what inhibits the ability for observational studies to make causal claims. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured.\n\n?fig-county-multi-unit-homeownership shows a negative association between the homeownership rate and the percentage of housing units that are in multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest a variable that might explain the negative relationship.10\n\nObservational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of patients over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses’ Health Study. Started in 1976 and expanded in 1989, the Nurses’ Health Study has collected data on over 275,000 nurses and is still enrolling participants. This prospective study recruits registered nurses and then collects data from them using questionnaires. Retrospective studies collect data after events have taken place, e.g., researchers may review past events in medical records. Some datasets may contain both prospectively- and retrospectively collected variables, such as medical studies which gather information on participants’ lives before they enter the study and subsequently collect data on participants throughout the study.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "content/02-data-design.html#sec-chp2-review",
    "href": "content/02-data-design.html#sec-chp2-review",
    "title": "2  Study design",
    "section": "2.4 Chapter review",
    "text": "2.4 Chapter review\n\n2.4.1 Summary\nA strong analyst will have a good sense of the types of data they are working with and how to visualize the data in order to gain a complete understanding of the variables. Equally important however, is an understanding of the data source. In this chapter, we have discussed randomized experiments and taking good, random, representative samples from a population. When we discuss inferential methods (starting in ?sec-foundations-randomization), the conclusions that can be drawn will be dependent on how the data were collected. ?fig-randsampValloc summarizes the differences between random assignment of treatments and random samples.11 Regularly revisiting ?fig-randsampValloc will be important when making conclusions from a given data analysis.\n\n\n2.4.2 Terms\nWe introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However, you should be able to easily spot them as bolded text.\n\n\n\n\nChance, Beth, and Allan Rossman. 2018. Investigating Statistical Concepts, Applications, and Methods. http://www.rossmanchance.com/iscam3/.\n\n\nRamsey, F., and D. Schafer. 2012. The Statistical Sleuth. 3rd ed. Cengage Learning.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "content/03-surah-An-Nisa.html#sec-case-study-paralympics",
    "href": "content/03-surah-An-Nisa.html#sec-case-study-paralympics",
    "title": "3  Applications: Data",
    "section": "3.1 Case study: Olympic 1500m",
    "text": "3.1 Case study: Olympic 1500m\n\nDo these data come from an observational study or an experiment?1\n\n\nThere are r nrow(paralympic_1500) rows and r ncol(paralympic_1500) columns in the dataset. What does each row and each column represent?2\n\n\nSimpson’s paradox.\nSimpson’s paradox happens when an association or relationship between two variables in one direction (e.g., positive) reverses (e.g., becomes negative) when a third variable is considered.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications: Data</span>"
    ]
  },
  {
    "objectID": "content/03-surah-An-Nisa.html#sec-data-tutorials",
    "href": "content/03-surah-An-Nisa.html#sec-data-tutorials",
    "title": "3  Applications: Data",
    "section": "3.2 Interactive R tutorials",
    "text": "3.2 Interactive R tutorials\nNavigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started!\n\nTutorial 1: Introduction to data\n\n\nTutorial 1 - Lesson 1: Language of data\n\n\nTutorial 1 - Lesson 2: Types of studies\n\n\nTutorial 1 - Lesson 3: Sampling strategies and experimental design\n\n\nTutorial 1 - Lesson 4: Case study\n\nYou can also access the full list of tutorials supporting this book here.\nYou can also access the full list of labs supporting this book here.",
    "crumbs": [
      "Introduction to data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications: Data</span>"
    ]
  },
  {
    "objectID": "content/intro2.html",
    "href": "content/intro2.html",
    "title": "Introduction to data part 2",
    "section": "",
    "text": "In this part of the book, you will be introduced to data, their properties, how they are collected, and the structure of the design used for the study. Different data and settings lead to different types of conclusions, so you’ll always want to keep in mind the data provenance, especially as you move on to modeling and inference.",
    "crumbs": [
      "Introduction to data part 2"
    ]
  },
  {
    "objectID": "content/04-explore-categorical.html#contingency-tables-and-bar-plots",
    "href": "content/04-explore-categorical.html#contingency-tables-and-bar-plots",
    "title": "4  Exploring categorical data",
    "section": "4.1 Contingency tables and bar plots",
    "text": "4.1 Contingency tables and bar plots\n?tbl-loan-home-app-type-totals summarizes two variables: application_type and homeownership. A table that summarizes data for two categorical variables in this way is called a contingency table. Each value in the table represents the number of times a particular combination of variable outcomes occurred.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring categorical data</span>"
    ]
  },
  {
    "objectID": "content/04-explore-categorical.html#visualizing-two-categorical-variables",
    "href": "content/04-explore-categorical.html#visualizing-two-categorical-variables",
    "title": "4  Exploring categorical data",
    "section": "4.2 Visualizing two categorical variables",
    "text": "4.2 Visualizing two categorical variables\n\n4.2.1 Bar plots with two variables\nWe can display the distributions of two categorical variables on a bar plot concurrently. Such plots are generally useful for visualizing the relationship between two categorical variables. ?fig-loan-homeownership-app-type-bar-plot shows three such plots that visualize the relationship between homeownership and application_type variables. Plot A in ?fig-loan-homeownership-app-type-bar-plot is a stacked bar plot. This plot most clearly displays that loan applicants most commonly live in mortgaged homes. It is difficult to say, based on this plot alone, how different application types vary across the levels of homeownership. Plot B is a dodged bar plot. This plot most clearly displays that within each level of homeownership, individual applications are more common than joint applications. Finally, plot C is a standardized bar plot (also known as filled bar plot). This plot most clearly displays that joint applications are most common among loans for applicants who live in mortgaged homes, compared to renters and owners. This type of visualization is helpful in understanding the fraction of individual or joint loan applications for borrowers in each level of homeownership. Additionally, since the proportions of joint and individual loans vary across the groups, we can conclude that the two variables are associated for this sample.\n\nExamine the three bar plots in ?fig-loan-homeownership-app-type-bar-plot. When is the stacked, dodged, or standardized bar plot the most useful?\n\nThe stacked bar plot is most useful when it’s reasonable to assign one variable as the explanatory variable (here homeownership) and the other variable as the response (here application_type) since we are effectively grouping by one variable first and then breaking it down by the others.\nDodged bar plots are more agnostic in their display about which variable, if any, represents the explanatory and which the response variable. It is also easy to discern the number of cases in each of the six different group combinations. However, one downside is that it tends to require more horizontal space; the narrowness of Plot B compared to the other two in ?fig-loan-homeownership-app-type-bar-plot makes the plot feel a bit cramped. Additionally, when two groups are of very different sizes, as we see in the group own relative to either of the other two groups, it is difficult to discern if there is an association between the variables.\nThe standardized stacked bar plot is helpful if the primary variable in the stacked bar plot is relatively imbalanced, e.g., the category has only a third of the observations in the category, making the simple stacked bar plot less useful for checking for an association. The major downside of the standardized version is that we lose all sense of how many cases each of the bars represents.\n\n\n\n4.2.2 Mosaic plots\nA mosaic plot is a visualization technique suitable for contingency tables that resembles a standardized stacked bar plot with the benefit that we still see the relative group sizes of the primary variable as well.\nTo get started in creating our first mosaic plot, we’ll break a square into columns for each category of the variable, with the result shown in Plot A of ?fig-loan-homeownership-type-mosaic-plot. Each column represents a level of homeownership, and the column widths correspond to the proportion of loans in each of those categories. For instance, there are fewer loans where the borrower is an owner than where the borrower has a mortgage. In general, mosaic plots use box areas to represent the number of cases in each category.\nPlot B in ?fig-loan-homeownership-type-mosaic-plot displays the relationship between homeownership and application type. Each column is split proportionally to the number of loans from individual and joint borrowers. For example, the second column represents loans where the borrower has a mortgage, and it was divided into individual loans (upper) and joint loans (lower). As another example, the bottom segment of the third column represents loans where the borrower owns their home and applied jointly, while the upper segment of this column represents borrowers who are homeowners and filed individually. We can again use this plot to see that the homeownership and application_type variables are associated, since some columns are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized stacked bar plot.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring categorical data</span>"
    ]
  },
  {
    "objectID": "content/04-explore-categorical.html#row-and-column-proportions",
    "href": "content/04-explore-categorical.html#row-and-column-proportions",
    "title": "4  Exploring categorical data",
    "section": "4.3 Row and column proportions",
    "text": "4.3 Row and column proportions\nIn the previous sections we inspected visualizations of two categorical variables in bar plots and mosaic plots. However, we have not discussed how the values in the bar and mosaic plots that show proportions are calculated. In this section we will investigate fractional breakdown of one variable in another and we can modify our contingency table to provide such a view. ?tbl-loan-home-app-type-row-proportions shows row proportions for ?tbl-loan-home-app-type-totals, which are computed as the counts divided by their row totals. The value 3496 at the intersection of individual and rent is replaced by \\(3496 / 8505 = 0.411,\\) i.e., 3496 divided by its row total, 8505. So, what does 0.411 represent? It corresponds to the proportion of individual applicants who rent.\nRow and column proportions can also be thought of as conditional proportions as they tell us about the proportion of observations in a given level of a categorical variable conditional on the level of another categorical variable.\nWe could also have checked for an association between application_type and homeownership in ?tbl-loan-home-app-type-row-proportions using row proportions. When comparing these row proportions, we would look down columns to see if the fraction of loans where the borrower rents, has a mortgage, or owns varied across the application types.\n\nWhat does 0.451 represent in ?tbl-loan-home-app-type-row-proportions? What does 0.802 represent in ?tbl-loan-home-app-type-column-proportions?1\n\n\nWhat does 0.122 represent in ?tbl-loan-home-app-type-row-proportions? What does 0.135 represent in ?tbl-loan-home-app-type-column-proportions?2\n\n\nData scientists use statistics to build email spam filters. By noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy. One such characteristic is whether the email contains no numbers, small numbers, or big numbers. Another characteristic is the email format, which indicates whether an email has any HTML content, such as bolded text. We’ll focus on email format and spam status using the dataset; these variables are summarized in a contingency table. Which would be more helpful to someone hoping to classify email as spam or regular email for this table: row or column proportions?\n\nA data scientist would be interested in how the proportion of spam changes within each email format. This corresponds to column proportions: the proportion of spam in plain text emails and the proportion of spam in HTML emails.\nIf we generate the column proportions, we can see that a higher fraction of plain text emails are spam (\\(209/1195 = 17.5\\%\\)) than compared to HTML emails (\\(158/2726 = 5.8\\%\\)). This information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam. Yet, when we carefully combine this information with many other characteristics, we stand a reasonable chance of being able to classify some emails as spam or not spam with confidence. This example points out that row and column proportions are not equivalent. Before settling on one form for a table, it is important to consider each to ensure that the most useful table is constructed. However, sometimes it simply isn’t clear which, if either, is more useful.\n\n\nThe email data can be found in the openintro R package.\n\n\nLook back to ?tbl-loan-home-app-type-row-proportions and ?tbl-loan-home-app-type-column-proportions. Are there any obvious scenarios where one might be more useful than the other?\n\nNone that we think are obvious! What is distinct about the email example is that the two loan variables do not have a clear explanatory-response variable relationship that we might hypothesize. Usually it is most useful to “condition” on the explanatory variable. For instance, in the email example, the email format was seen as a possible explanatory variable of whether the message was spam, so we would find it more interesting to compute the relative frequencies (proportions) for each email format.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring categorical data</span>"
    ]
  },
  {
    "objectID": "content/04-explore-categorical.html#pie-charts",
    "href": "content/04-explore-categorical.html#pie-charts",
    "title": "4  Exploring categorical data",
    "section": "4.4 Pie charts",
    "text": "4.4 Pie charts\nA pie chart is shown in ?fig-loan-homeownership-pie-chart alongside a bar plot representing the same information. Pie charts can be useful for giving a high-level overview to show how a set of cases break down. However, it is also difficult to decipher certain details in a pie chart. For example, it’s not immediately obvious that there are more loans where the borrower has a mortgage than rent when looking at the pie chart, while this detail is very obvious in the bar plot.\nPie charts can work well when the goal is to visualize a categorical variable with very few levels, and especially if each level represents a simple fraction (e.g., one-half, one-quarter, etc.). However, they can be quite difficult to read when they are used to visualize a categorical variable with many levels. For example, the pie chart and the bar plot in ?fig-loan-grade-pie-chart both represent the distribution of loan grades (A through G). In this case, it is far easier to compare the counts of each loan grade using the bar plot than the pie chart.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring categorical data</span>"
    ]
  },
  {
    "objectID": "content/04-explore-categorical.html#comparing-numerical-data-across-groups",
    "href": "content/04-explore-categorical.html#comparing-numerical-data-across-groups",
    "title": "4  Exploring categorical data",
    "section": "4.5 Comparing numerical data across groups",
    "text": "4.5 Comparing numerical data across groups\nSome of the more interesting investigations can be considered by examining numerical data across groups. In this section we will expand on a few methods we have already seen to make plots for numerical data from multiple groups on the same graph as well as introduce a few new methods for comparing numerical data across groups.\nWe will revisit the county dataset and compare the median household income for counties that gained population from 2010 to 2017 versus counties that had no gain. While we might like to make a causal connection between income and population growth, remember that these are observational data and so such an interpretation would be, at best, half-baked.\nColor can be used to split histograms (see Section 5.2 for an introduction to histograms) for numerical variables by levels of a categorical variable. An example of this is shown in Plot A of ?fig-countyIncomeSplitByPopGain. The side-by-side box plot is another traditional tool for comparing across groups. An example is shown in Plot B of ?fig-countyIncomeSplitByPopGain, where there are two box plots (see Section 5.4 for an introduction to box plots), one for each group, placed into one plotting window and drawn on the same scale.\n\nUse the plots in ?fig-countyIncomeSplitByPopGain to compare the incomes for counties across the two groups. What do you notice about the approximate center of each group? What do you notice about the variability between groups? Is the shape relatively consistent between groups? How many prominent modes are there for each group?3\n\n\nWhat components of each plot in ?fig-countyIncomeSplitByPopGain do you find most useful?4\n\nAnother useful visualization for comparing numerical data across groups is a ridge plot, which combines density plots (see Section 5.4 for an introduction to density plots) for various groups drawn on the same scale in a single plotting window. ?fig-countyIncomeSplitByPopGainRidge displays a ridge plot for the distribution of median household income in counties, split by whether there was a population gain or not.\n\nWhat components of the ridge plot in ?fig-countyIncomeSplitByPopGainRidge do you find most useful compared to those in ?fig-countyIncomeSplitByPopGain?5\n\nOne last visualization technique we’ll highlight for comparing numerical data across groups is faceting. In this technique we split (facet) the graphical display of the data across plotting windows based on groups. Plot A in ?fig-countyIncomeSplitByPopGainFacetHist displays the same information as Plot A in ?fig-countyIncomeSplitByPopGain, however here the distributions of median household income for counties with and without population gain are faceted across two plotting windows. We preserve the same scale on the x and y axes for easier comparison. An advantage of this approach is that it extends to splitting the data across levels of two categorical variables, which allows for displaying relationships between three variables. In Plot B in ?fig-countyIncomeSplitByPopGainFacetHist we have now split the data into four groups using the pop_change and metro variables:\n\ntop left represents counties that are not in a metropolitan area with population gain,\ntop right represents counties that are in a metropolitan area with population gain,\nbottom left represents counties that are not in a metropolitan area without population gain, and finally\nbottom right represents counties that are in a metropolitan area without population gain.\n\n\nBased on ?fig-countyIncomeRidgeMulti, what can you say about how median household income in counties vary depending on population gain/no gain, metropolitan area/not, and median degree?6",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring categorical data</span>"
    ]
  },
  {
    "objectID": "content/04-explore-categorical.html#sec-chp4-review",
    "href": "content/04-explore-categorical.html#sec-chp4-review",
    "title": "4  Exploring categorical data",
    "section": "4.6 Chapter review",
    "text": "4.6 Chapter review\n\n4.6.1 Summary\nFluently working with categorical variables is an important skill for data analysts. In this chapter we have introduced different visualizations and numerical summaries applied to categorical variables. The graphical visualizations are even more descriptive when two variables are presented simultaneously. We presented bar plots, mosaic plots, pie charts, and estimations of conditional proportions.\n\n\n4.6.2 Terms\nWe introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However, you should be able to easily spot them as bolded text.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploring categorical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#sec-dotplots",
    "href": "content/05-explore-numerical.html#sec-dotplots",
    "title": "5  Exploring numerical data",
    "section": "5.1 Dot plots and the mean",
    "text": "5.1 Dot plots and the mean\nSometimes we are interested in the distribution of a single variable. In these cases, a dot plot provides the most basic of displays.\nMean.\nThe sample mean can be calculated as the sum of the observed values divided by the number of observations:\nThe loan50 dataset represents a sample from a larger population of loans made through Lending Club. We could compute a mean for the entire population in the same way as the sample mean. However, the population mean has a special label: \\(\\mu.\\) The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x,\\) is used to represent which variable the population mean refers to, e.g., \\(\\mu_x.\\) Oftentimes it is too expensive to measure the population mean precisely, so we often estimate \\(\\mu\\) using the sample mean, \\(\\bar{x}.\\)\n\nThe Greek letter \\(\\mu\\) is pronounced mu, listen to the pronunciation here.\n\n::: {.workedexample data-latex=““} Although we do not have an ability to calculate the average interest rate across all loans in the populations, we can estimate the population value using the sample data. Based on the sample of 50 loans, what would be a reasonable estimate of \\(\\mu_x,\\) the mean interest rate for all loans in the full dataset?\n\nThe mean is useful because it allows us to rescale or standardize a metric into something more easily interpretable and comparable. Suppose we would like to understand if a new drug is more effective at treating asthma attacks than the standard drug. A trial of 1,500 adults is set up, where 500 receive the new drug, and 1000 receive a standard drug in the control group. Results of this trial are summarized in ?tbl-drug-asthma-results.\nComparing the raw counts of 200 to 300 asthma attacks would make it appear that the new drug is better, but this is an artifact of the imbalanced group sizes. Instead, we should look at the average number of asthma attacks per patient in each group:\n\nNew drug: \\(200 / 500 = 0.4\\) asthma attacks per patient\nStandard drug: \\(300 / 1000 = 0.3\\) asthma attacks per patient\n\nThe standard drug has a lower average number of asthma attacks per patient than the average in the treatment group.\n\nCome up with another example where the mean is useful for making comparisons.\n\nEmilio opened a food truck last year where he sells burritos, and his business has stabilized over the last 3 months. Over that 3-month period, he has made $11,000 while working 625 hours. Emilio’s average hourly earnings provides a useful statistic for evaluating whether his venture is, at least from a financial perspective, worth it:\n\\[ \\frac{\\$11000}{625\\text{ hours}} = \\$17.60\\text{ per hour} \\]\nBy knowing his average hourly wage, Emilio now has put his earnings into a standard unit that is easier to compare with many other jobs that he might consider.\n\n\nSuppose we want to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes across the 3,142 counties in the county dataset. What would be a better approach?\n\nThe county dataset is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. If we completed these steps with the county data, we would find that the per capita income for the US is $30,861. Had we computed the simple mean of per capita income across counties, the result would have been just $26,093!\nThis example used what is called a weighted mean. For more information on this topic, check out the following online supplement regarding weighted means.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#sec-histograms",
    "href": "content/05-explore-numerical.html#sec-histograms",
    "title": "5  Exploring numerical data",
    "section": "5.2 Histograms and shape",
    "text": "5.2 Histograms and shape\nDot plots show the exact value for each observation. They are useful for small datasets but can become hard to read with larger samples. Rather than showing the value of each observation, we prefer to think of the value as belonging to a bin. For example, in the loan50 dataset, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on. Observations that fall on the boundary of a bin (e.g., 10.00%) are allocated to the lower bin. The tabulation is shown in ?tbl-binnedIntRateAmountTable, and the binned counts are plotted as bars into what is called a histogram. Note that the histogram resembles a more heavily binned version of the stacked dot plot shown in ?fig-loan-int-rate-dotplot.\nHistograms provide a view of the data density. Higher bars represent where the data are relatively more common. For instance, there are many more loans with rates between 5% and 10% than loans with rates between 20% and 25% in the dataset. The bars make it easy to see how the density of the data changes relative to the interest rate.\nHistograms are especially convenient for understanding the shape of the data distribution. suggests that most loans have rates under 15%, while only a handful of loans have rates above 20%. When the distribution of a variable trails off to the right in this way and has a longer right tail, the shape is said to be right skewed.3\nVariables with the reverse characteristic – a long, thinner tail to the left – are said to be left skewed. We also say that such a distribution has a long left tail. Variables that show roughly equal trailing off in both directions are called symmetric.\n\nWhen data trail off in one direction, the distribution has a long tail. If a distribution has a long left tail, it is left skewed. If a distribution has a long right tail, it is right skewed.\n\nIn addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution. There is only one prominent peak in the histogram of interest_rate.\nA definition of mode sometimes taught in math classes is the value with the most occurrences in the dataset. However, for many real-world datasets, it is common to have no observations with the same value in a dataset, making this definition impractical in data analysis.\n?fig-singleBiMultiModalPlots shows histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than two prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#sec-variance-sd",
    "href": "content/05-explore-numerical.html#sec-variance-sd",
    "title": "5  Exploring numerical data",
    "section": "5.3 Variance and standard deviation",
    "text": "5.3 Variance and standard deviation\nThe mean was introduced as a method to describe the center of a variable, and variability in the data is also important. Here, we introduce two measures of variability: the variance and the standard deviation. Both of these are very useful in data analysis, even though their formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to comprehend, as it roughly describes how far away the typical observation is from the mean.\nWe call the distance of an observation from its mean its deviation. Below are the deviations for the \\(1^{st},\\) \\(2^{nd},\\) \\(3^{rd},\\) and \\(50^{th}\\) observations in the interest_rate variable:\nWe divide by \\(n - 1,\\) rather than dividing by \\(n,\\) when computing a sample’s variance. There’s some mathematical nuance here, but the end result is that doing this makes this statistic slightly more reliable and useful.\nNotice that squaring the deviations does two things. First, it makes large values relatively much larger. Second, it gets rid of any negative signs.\n\nStandard deviation.\nThe sample standard deviation can be calculated as the square root of the sum of the squared distance of each value from the mean divided by the number of observations minus one:\n\\[s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}}\\]\n\nThe standard deviation is defined as the square root of the variance:\nWhile often omitted, a subscript of \\(_x\\) may be added to the variance and standard deviation, i.e., \\(s_x^2\\) and \\(s_x^{},\\) if it is useful as a reminder that these are the variance and standard deviation of the observations represented by \\(x_1,\\) \\(x_2,\\) …, \\(x_n.\\)\n\nVariance and standard deviation.\nThe variance is the average squared distance from the mean. The standard deviation is the square root of the variance. The standard deviation is useful when considering how far the data are distributed from the mean.\nThe standard deviation represents the typical deviation of observations from the mean. Often about 68% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, these percentages are not strict rules.\n\nLike the mean, the population values for variance and standard deviation have special symbols: \\(\\sigma^2\\) for the variance and \\(\\sigma\\) for the standard deviation.\n\nThe Greek letter \\(\\sigma\\) is pronounced sigma, listen to the pronunciation here.\n\n\nA good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using ?fig-severalDiffDistWithSdOf1 as an example, explain why such a description is important.4\n\n\nDescribe the distribution of the interest_rate variable using the histogram in. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context. Also note any especially unusual cases.\n\nThe distribution of interest rates is unimodal and skewed to the high end. Many of the rates fall near the mean at 11.57%, and most fall within one standard deviation (5.05%) of the mean. There are a few exceptionally large interest rates in the sample that are above 20%.\n\nIn practice, the variance and standard deviation are sometimes used as a means to an end, where the “end” is being able to accurately estimate the uncertainty associated with a sample statistic. For example, in ?sec-foundations-mathematical the standard deviation is used in calculations that help us understand how much a sample mean varies from one sample to the next.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#sec-boxplots",
    "href": "content/05-explore-numerical.html#sec-boxplots",
    "title": "5  Exploring numerical data",
    "section": "5.4 Box plots, quartiles, and the median",
    "text": "5.4 Box plots, quartiles, and the median\nThe dark line inside the box represents the median, which splits the data in half. 50% of the data fall below this value and 50% fall above it.\n\nMedian: the number in the middle.\nIf the data are ordered from smallest to largest, the median is the observation right in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average.\n\nThe second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The length of the box is called the interquartile range, or IQR for short. It, like the standard deviation, is a measure of variability in data. The more variable the data, the larger the standard deviation and IQR tend to be. The two boundaries of the box are called the first quartile (the \\(25^{th}\\) percentile, i.e., 25% of the data fall below this value) and the third quartile (the \\(75^{th}\\) percentile, i.e., 75% of the data fall below this value), and these are often labeled \\(Q_1\\) and \\(Q_3,\\) respectively.\nThe median and IQR are called robust statistics because extreme observations have little effect on their values: moving the most extreme value generally has little influence on these statistics. On the other hand, the mean and standard deviation are more heavily influenced by changes in extreme observations, which can be important in some situations.\n\nThe median and IQR did not change under the three scenarios in ?tbl-robustOrNotTable. Why might this be the case?\n\nThe median and IQR are only sensitive to numbers near \\(Q_1,\\) the median, and \\(Q_3.\\) Since values in these regions are stable in the three datasets, the median and IQR estimates are also stable.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#sec-transforming-data",
    "href": "content/05-explore-numerical.html#sec-transforming-data",
    "title": "5  Exploring numerical data",
    "section": "5.5 Transforming data",
    "text": "5.5 Transforming data\nWhen data are very strongly skewed, we sometimes transform them, so they are easier to model. ?fig-county-unemployed-pop-transform shows two right skewed distributions: distribution of the percentage of unemployed people and the distribution of the population in all counties in the United States. The distribution of population is more strongly skewed than the distribution of unemployed, hence the log transformation results in a much bigger change in the shape of the distribution.\n\nConsider the histogram of county populations shown in Plot C of ?fig-county-unemployed-pop-transform, which shows extreme skew. What characteristics of the plot keep it from being useful?\n\nNearly all of the data fall into the left-most bin, and the extreme skew obscures many of the potentially interesting details at the low values.\n\nThere are some standard transformations that may be useful for strongly right skewed data where much of the data is positive but clustered near zero. A transformation is a rescaling of the data using a function. For instance, a plot of the logarithm (base 10) of unemployment rates and county populations results in the new histograms on the right in ?fig-county-unemployed-pop-transform. The transformed data are symmetric, and any potential outliers appear much less extreme than in the original data set. By reigning in the outliers and extreme skew, transformations often make it easier to build statistical models for the data.\nTransformations other than the logarithm can be useful, too. For instance, the square root \\((\\sqrt{\\text{original observation}})\\) and inverse \\(\\bigg ( \\frac{1}{\\text{original observation}} \\bigg )\\) are commonly used by data scientists. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#mapping-data",
    "href": "content/05-explore-numerical.html#mapping-data",
    "title": "5  Exploring numerical data",
    "section": "5.6 Mapping data",
    "text": "5.6 Mapping data\n\nThe county dataset offers many numerical variables that we could plot using dot plots, scatterplots, or box plots, but they can miss the true nature of the data as geographic. When we encounter geographic data, we should create an intensity map, where colors are used to show higher and lower values of a variable. Figures ?fig-county-intensity-map-poverty-unemp and ?fig-county-intensity-map-howownership-median-income show intensity maps for poverty rate in percent (poverty), unemployment rate in percent (unemployment_rate), homeownership rate in percent (homeownership), and median household income in $1000s (median_hh_income). The color key indicates which colors correspond to which values. The intensity maps are not generally very helpful for getting precise values in any given county, but they are very helpful for seeing geographic trends and generating interesting research questions or hypotheses.\n\nWhat interesting features are evident in the poverty and unemployment rate intensity maps?\n\nPoverty rates are evidently higher in a few locations. Notably, the deep south shows higher poverty rates, as does much of Arizona and New Mexico. High poverty rates are evident in the Mississippi flood plains a little north of New Orleans and in a large section of Kentucky.\nThe unemployment rate follows similar trends, and we can see correspondence between the two variables. In fact, it makes sense for higher rates of unemployment to be closely related to poverty rates. One observation that stands out when comparing the two maps: the poverty rate is much higher than the unemployment rate, meaning while many people may be working, they are not making enough to break out of poverty.\n\n\nWhat interesting features are evident in the median household income intensity map in ?fig-county-intensity-map-howownership-median-income?5",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/05-explore-numerical.html#sec-chp5-review",
    "href": "content/05-explore-numerical.html#sec-chp5-review",
    "title": "5  Exploring numerical data",
    "section": "5.7 Chapter review",
    "text": "5.7 Chapter review\n\n5.7.1 Summary\nFluently working with numerical variables is an important skill for data analysts. In this chapter we have introduced different visualizations and numerical summaries applied to numeric variables. The graphical visualizations are even more descriptive when two variables are presented simultaneously. We presented scatterplots, dot plots, histograms, and box plots. Numerical variables can be summarized using the mean, median, quartiles, standard deviation, and variance.\n\n\n5.7.2 Terms\nWe introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However, you should be able to easily spot them as bolded text.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploring numerical data</span>"
    ]
  },
  {
    "objectID": "content/06-explore-applications.html#sec-case-study-effective-comms",
    "href": "content/06-explore-applications.html#sec-case-study-effective-comms",
    "title": "6  Applications: Explore",
    "section": "6.1 Case study: Effective communication of exploratory results",
    "text": "6.1 Case study: Effective communication of exploratory results\nGraphs can powerfully communicate ideas directly and quickly. We all know, after all, that “a picture is worth 1000 words.” Unfortunately, however, there are times when an image conveys a message which is inaccurate or misleading.\nThis chapter focuses on how graphs can best be utilized to present data accurately and effectively. Along with data modeling, creative visualization is somewhat of an art. However, even with an art, there are recommended guiding principles. We provide a few best practices for creating data visualizations.\n\n6.1.1 Keep it simple\nWhen creating a graphic, keep in mind what it is that you’d like your reader to see. Colors should be used to group items or differentiate levels in meaningful ways. Colors can be distracting when they are only used to brighten up the plot.\nConsider a manufacturing company that has summarized its costs into five different categories. In the two graphics provided in ?fig-pie-to-bar, notice that the magnitudes in the pie chart in ?fig-pie-to-bar-1 are difficult for the eye to compare. That is, can your eye tell how different “Buildings and administration” is from “Workplace materials” when looking at the slices of pie? Additionally, the colors in the pie chart do not mean anything and are therefore distracting. Lastly, the three-dimensional aspect of the image does not improve the reader’s ability to understand the data presented.\nAs an alternative, a bar plot is been provided in ?fig-pie-to-bar-2. Notice how much easier it is to identify the magnitude of the differences across categories while not being distracted by other aspects of the image. Typically, a bar plot will be easier for the reader to digest than a pie chart, especially if the categorical data being plotted has more than just a few levels.\n\n\n6.1.2 Use color to draw attention\nThere are many reasons why you might choose to add color to your plots. An important principle to keep in mind is to use color to draw attention. Of course, you should still think about how visually pleasing your visualization is, and if you’re adding color for making it visually pleasing without drawing attention to a particular feature, that might be fine. However, you should be critical of default coloring and explicitly decide whether to include color and how. Notice that in Plot B in ?fig-red-bar the coloring is done in such a way to draw the reader’s attention to one particular piece of information. The default coloring in Plot A can be distracting and makes the reader question, for example, is there something similar about the red and purple bars? Also note that not everyone sees color the same way, often it’s useful to add color and one more feature (e.g., pattern) so that you can refer to the features you’re drawing attention to in multiple ways.\n\n\n6.1.3 Tell a story\nFor many graphs, an important aspect is the inclusion of information which is not provided in the dataset that is being plotted. The external information serves to contextualize the data and helps communicate the narrative of the research. In ?fig-duke-hires, the graph on the right is annotated with information about the start of the university’s fiscal year which contextualizes the information provided by the data. Sometimes the additional information may be a diagonal line given by \\(y = x\\), points above the line quickly show the reader which values have a \\(y\\) coordinate larger than the \\(x\\) coordinate; points below the line show the opposite.\n\n\n6.1.4 Order matters\nMost software programs have built in methods for some of the plot details. For example, the default option for the software program used in this text, R, is to order the bars in a bar plot alphabetically. As seen in ?fig-brexit-bars, the alphabetical ordering isn’t particularly meaningful for describing the data. Sometimes it makes sense to order the bars from tallest to shortest (or vice versa). But in this case, the best ordering is probably the one in which the questions were asked. An ordering which does not make sense in the context of the problem (e.g., alphabetically here), can mislead the reader who might take a quick glance at the axes and not read the bar labels carefully.\nIn September 2019, YouGov survey asked 1,639 Great Britain adults the following question1:\n\nHow well or badly do you think the government are doing at handling Britain’s exit from the European Union?\n\nVery well\nFairly well\nFairly badly\nVery badly\nDon’t know\n\n\n\n\n6.1.5 Pick a purpose\nEvery graphical decision should be made with a purpose. As previously mentioned, sticking with default options is not always best for conveying the narrative of your data story. Stacked bar plots tell one part of a story. Depending on your research question, they may not tell the part of the story most important to the research. ?fig-seg-three-ways provides three different ways of representing the same information. If the most important comparison across regions is proportion, you might prefer Plot A. If the most important comparison across regions also considers the total number of individuals in the region, you might prefer Plot B. If a separate bar plot for each region makes the point you’d like, use Plot C, which has been faceted by region.\n\n\n6.1.6 Select meaningful colors\n\nOne last consideration for building graphs is to consider color choices. Default or rainbow colors are not always the choice which will best distinguish the level of your variables. Much research has been done to find color combinations which are distinct and which are clear for differently sighted individuals. The cividis scale works well with ordinal data. (Nuñez, Anderton, and Renslow 2018) ?fig-brexit-viridis shows the same plot with two different color themes.\nIn this chapter different representations are contrasted to demonstrate best practices in creating graphs. The fundamental principle is that your graph should provide maximal information succinctly and clearly. Labels should be clear and oriented horizontally for the reader. Don’t forget titles and, if possible, include the source of the data.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applications: Explore</span>"
    ]
  },
  {
    "objectID": "content/06-explore-applications.html#sec-explore-tutorials",
    "href": "content/06-explore-applications.html#sec-explore-tutorials",
    "title": "6  Applications: Explore",
    "section": "6.2 Interactive R tutorials",
    "text": "6.2 Interactive R tutorials\nNavigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started!\n\nTutorial 2: Exploratory data analysis\n\n\nTutorial 2 - Lesson 1: Visualizing categorical data\n\n\nTutorial 2 - Lesson 2: Visualizing numerical data\n\n\nTutorial 2 - Lesson 3: Summarizing with statistics\n\n\nTutorial 2 - Lesson 4: Case study\n\nYou can also access the full list of tutorials supporting this book here.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applications: Explore</span>"
    ]
  },
  {
    "objectID": "content/06-explore-applications.html#sec-explore-labs",
    "href": "content/06-explore-applications.html#sec-explore-labs",
    "title": "6  Applications: Explore",
    "section": "6.3 R labs",
    "text": "6.3 R labs\nFurther apply the concepts you’ve learned in this part in R with computational labs that walk you through a data analysis case study.\n\nIntro to data - Flight delays\n\n\nYou can also access the full list of labs supporting this book here.\n\n\n\n\nNuñez, Jamie R, Christopher R Anderton, and Ryan S Renslow. 2018. “Optimizing Colormaps with Consideration for Color Vision Deficiency to Enable Accurate Interpretation of Scientific Data.” PloS One 13 (7). https://doi.org/10.1371/journal.pone.0199239.",
    "crumbs": [
      "Introduction to data part 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Applications: Explore</span>"
    ]
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "Appendix A — References",
    "section": "",
    "text": "Chance, Beth, and Allan Rossman. 2018. Investigating Statistical Concepts, Applications, and\nMethods. http://www.rossmanchance.com/iscam3/.\n\n\nChimowitz, Marc I, Michael J Lynn, Colin P Derdeyn, Tanya N Turan, David\nFiorella, Bethany F Lane, L Scott Janis, et al. 2011. “Stenting\nVersus Aggressive Medical Therapy for Intracranial Arterial\nStenosis.” New England Journal of Medicine 365 (11):\n993–1003. http://www.nejm.org/doi/full/10.1056/NEJMoa1105335.\n\n\nNuñez, Jamie R, Christopher R Anderton, and Ryan S Renslow. 2018.\n“Optimizing Colormaps with Consideration for Color Vision\nDeficiency to Enable Accurate Interpretation of Scientific Data.”\nPloS One 13 (7). https://doi.org/10.1371/journal.pone.0199239.\n\n\nRamsey, F., and D. Schafer. 2012. The Statistical Sleuth. 3rd\ned. Cengage Learning.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>References</span>"
    ]
  }
]